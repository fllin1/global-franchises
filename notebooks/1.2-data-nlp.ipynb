{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7170b547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from src.config import RAW_DATA_DIR\n",
    "from src.api.config.text_embedding_3_small_config import BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa370bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "franchises_data_dir = RAW_DATA_DIR / \"franserve\"\n",
    "franchises_data_files = list(franchises_data_dir.glob(\"*.json\"))\n",
    "\n",
    "columns = [\n",
    "    \"franchise_name\",\n",
    "    \"primary_category\",\n",
    "    \"sub_categories\",\n",
    "    \"why_franchise_summary\",\n",
    "    \"ideal_candidate_profile_text\",\n",
    "    \"description_text\",\n",
    "]\n",
    "offset = 0\n",
    "\n",
    "franchises_batch = []\n",
    "for file in franchises_data_files[offset : offset + BATCH_SIZE]:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        franchise_data = json.load(f)[\"franchise_data\"]\n",
    "        franchise_data = {k: v for k, v in franchise_data.items() if k in columns}\n",
    "    franchises_batch.append(franchise_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f38dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "franchises_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c182e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "\n",
    "from src.api.config.gemini_config import (\n",
    "    CLIENT,\n",
    "    MODEL_FLASH,\n",
    "    get_generate_content_config_keywords,\n",
    "    get_thinking_config,\n",
    "    get_tools,\n",
    ")\n",
    "from src.api.google_gemini import generate\n",
    "from src.config import CONFIG_DIR, RAW_DATA_DIR\n",
    "\n",
    "# --- Step 1: Create the Summary from Structured Data ---\n",
    "\n",
    "\n",
    "def create_summary_for_keyword_extraction(franchise_data: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Creates a clean, text-based summary from the structured franchise data\n",
    "    to be used as input for keyword extraction.\n",
    "\n",
    "    Args:\n",
    "        franchise_data: The dictionary of data extracted by the first LLM call.\n",
    "\n",
    "    Returns:\n",
    "        A single string summarizing the franchise.\n",
    "    \"\"\"\n",
    "    summary_parts = []\n",
    "\n",
    "    # Helper to add a field to the summary if it exists\n",
    "    def add_to_summary(key: str, label: str):\n",
    "        value = franchise_data.get(key)\n",
    "        if value:\n",
    "            # If value is a list (like from JSON), join it\n",
    "            if isinstance(value, list):\n",
    "                value_str = \", \".join(value)\n",
    "                summary_parts.append(f\"{label}: {value_str}\")\n",
    "            else:\n",
    "                summary_parts.append(f\"{label}: {value}\")\n",
    "\n",
    "    add_to_summary(\"franchise_name\", \"Franchise Name\")\n",
    "    add_to_summary(\"description_text\", \"Description\")\n",
    "    add_to_summary(\"why_franchise_summary\", \"Key Benefits\")\n",
    "    add_to_summary(\"ideal_candidate_profile_text\", \"Ideal Candidate\")\n",
    "\n",
    "    # Create a composite investment summary\n",
    "    min_inv = franchise_data.get(\"total_investment_min_usd\")\n",
    "    max_inv = franchise_data.get(\"total_investment_max_usd\")\n",
    "    if min_inv and max_inv:\n",
    "        summary_parts.append(f\"Total Investment: ${min_inv:,} - ${max_inv:,}\")\n",
    "\n",
    "    # Create a composite ownership model summary\n",
    "    ownership_model = []\n",
    "    if franchise_data.get(\"is_home_based\"):\n",
    "        ownership_model.append(\"Home-Based\")\n",
    "    if franchise_data.get(\"allows_absentee\"):\n",
    "        ownership_model.append(\"Absentee Ownership\")\n",
    "    elif franchise_data.get(\"allows_semi_absentee\"):\n",
    "        ownership_model.append(\"Semi-Absentee Ownership\")\n",
    "    if franchise_data.get(\"e2_visa_friendly\"):\n",
    "        ownership_model.append(\"E2 Visa Friendly\")\n",
    "\n",
    "    if ownership_model:\n",
    "        summary_parts.append(f\"Business Model: {', '.join(ownership_model)}\")\n",
    "\n",
    "    return \"\\n\".join(summary_parts)\n",
    "\n",
    "\n",
    "# --- Step 2: Call the LLM and Extract Keywords ---\n",
    "\n",
    "\n",
    "def extract_keywords_with_llm(\n",
    "    client: genai.Client,\n",
    "    model: str,\n",
    "    prompt: str,\n",
    "    franchise_summary: str,\n",
    ") -> List[str] | None:\n",
    "    \"\"\"\n",
    "    Calls the Gemini API to extract keywords from a franchise summary.\n",
    "\n",
    "    Args:\n",
    "        client: The initialized genai.Client.\n",
    "        model: The name of the model to use (e.g., 'gemini-1.5-pro-latest').\n",
    "        prompt: The keyword extraction prompt.\n",
    "        franchise_summary: The text summary of the franchise.\n",
    "\n",
    "    Returns:\n",
    "        A list of keywords, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    parts = [\n",
    "        types.Part(text=prompt),\n",
    "        types.Part(text=\"\\n--- FRANCHISE SUMMARY ---\\n\"),\n",
    "        types.Part(text=franchise_summary),\n",
    "    ]\n",
    "\n",
    "    generate_content_config = get_generate_content_config_keywords(\n",
    "        thinking_config=get_thinking_config(thinking_budget=-1),\n",
    "        tools=get_tools(google_search=True, url_context=True),\n",
    "    )\n",
    "\n",
    "    response = generate(\n",
    "        client=client,\n",
    "        model=model,\n",
    "        parts=parts,\n",
    "        generate_content_config=generate_content_config,\n",
    "    )\n",
    "\n",
    "    # Print token usage for cost estimation\n",
    "    if hasattr(response, \"usage_metadata\") and response.usage_metadata:\n",
    "        input_tokens = response.usage_metadata.prompt_token_count\n",
    "        output_tokens = response.usage_metadata.candidates_token_count\n",
    "        logger.info(f\"Token usage - Input: {input_tokens}, Output: {output_tokens}\")\n",
    "    else:\n",
    "        logger.warning(\"Token usage information not available in response\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d21f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the keyword extraction pipeline with upsert support.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load prompt\n",
    "    prompt_path = CONFIG_DIR / \"franserve\" / \"keywords_prompt.txt\"\n",
    "    with open(prompt_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        prompt_keywords = file.read()\n",
    "\n",
    "    # Process each franchise\n",
    "    franchise_data_paths = list((RAW_DATA_DIR / \"franserve\").glob(\"*.json\"))\n",
    "    logger.info(f\"Found {len(franchise_data_paths)} franchise files.\")\n",
    "\n",
    "    for franchise_data_path in franchise_data_paths:\n",
    "        logger.debug(f\"Processing {franchise_data_path.name}\")\n",
    "\n",
    "        with open(franchise_data_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            raw_data = json.load(file)\n",
    "            franchise_data = raw_data[\"franchise_data\"]\n",
    "\n",
    "        # Generate inputs\n",
    "        franchise_summary = create_summary_for_keyword_extraction(franchise_data)\n",
    "        response = extract_keywords_with_llm(\n",
    "            client=CLIENT,\n",
    "            model=MODEL_FLASH,\n",
    "            prompt=prompt_keywords,\n",
    "            franchise_summary=franchise_summary,\n",
    "        )\n",
    "\n",
    "        logger.success(f\"Processed {franchise_data.get('source_id')}\")\n",
    "\n",
    "        break\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1470f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69912b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cce936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.nlp.genai_keywords_batch import check_keywords_batch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f41fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_keywords_batch_results(\"batches/uh6g31hv1o06t7kslrzy738lkipqw5qjt9pg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_franchises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
