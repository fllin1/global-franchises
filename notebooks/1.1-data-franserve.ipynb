{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99418782",
   "metadata": {},
   "source": [
    "# FranServe Data\n",
    "\n",
    "_Production Code: ./src/data/franserve/\\*.py_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f1bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import dotenv\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872eb9d7",
   "metadata": {},
   "source": [
    "We need to log into an account to have access to the broker's data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "username = os.getenv(\"FRANSERVE_EMAIL\")\n",
    "password = os.getenv(\"FRANSERVE_PASSWORD\")\n",
    "\n",
    "LOGIN_URL = \"https://franservesupport.com/Default.asp\"\n",
    "PROTECTED_URL = \"https://franservesupport.com/directory.asp?ClientID=\"\n",
    "\n",
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12c9aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Fetch the login page to grab any hidden form tokens (CSRF, etc.)\n",
    "resp = session.get(LOGIN_URL)\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "# 2) Post your credentials to log in\n",
    "LOGIN_ACTION = \"https://franservesupport.com/process_login.asp\"\n",
    "\n",
    "\n",
    "payload = {\n",
    "    \"email\": username,\n",
    "    \"password\": password,\n",
    "    \"Submit\": \"Login\",\n",
    "}\n",
    "login_resp = session.post(LOGIN_ACTION, data=payload)\n",
    "login_resp.raise_for_status()  # ensure login succeeded\n",
    "\n",
    "print(\"After login, URL is:\", login_resp.url)\n",
    "print(\"Redirect history:\", [r.status_code for r in login_resp.history])\n",
    "\n",
    "# 3) Now session holds the auth cookiesâ€”fetch protected content\n",
    "protected = session.get(PROTECTED_URL)\n",
    "protected.raise_for_status()\n",
    "print(\"Protected page title:\", BeautifulSoup(protected.text, \"html.parser\").title.string)\n",
    "\n",
    "# Expected output:\n",
    "#\n",
    "# After login, URL is: https://franservesupport.com/main.asp?login=1\n",
    "# Redirect history: [302]\n",
    "# Protected page title: FranServe Franchise Portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bf672",
   "metadata": {},
   "source": [
    "The catalogues where we scrap the franchises' data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd14ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url: https://franservesupport.com/directory.asp?ClientID=\n",
    "# next_url: https://franservesupport.com/directory.asp?ClientID=&offset=50\n",
    "# next_url: https://franservesupport.com/directory.asp?ClientID=&offset=100\n",
    "# ...\n",
    "# last_url: https://franservesupport.com/directory.asp?ClientID=&offset=800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef1d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://franservesupport.com/\"\n",
    "CATALOGUE_BASE_URL = BASE_URL + \"directory.asp?ClientID=\"\n",
    "\n",
    "\n",
    "def get_franchise_url(session: requests.Session, base_url: str, catalogue_url: str) -> str:\n",
    "    resp = session.get(catalogue_url)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    matching_links = [\n",
    "        base_url + a['href']\n",
    "        for a in soup.find_all(\"a\", href=True)\n",
    "        if a['href'].startswith(\"franchisedetails\")\n",
    "    ]\n",
    "     \n",
    "    # We ignore the first link as it belongs to the ad at the top of the page\n",
    "    return matching_links[1:]\n",
    "\n",
    "\n",
    "def get_franchise_url_list(session: requests.Session, base_url: str, catalogue_base_url: str, offset_max: int = 800, offset_step: int = 50) -> list[str]:\n",
    "    \"\"\"\n",
    "    Get a list of URLs to scrape.\n",
    "\n",
    "    Args:\n",
    "        session (requests.Session): The session object to use.\n",
    "        base_url (str): The base URL to scrape.\n",
    "        catalogue_base_url (str): The base URL to scrape.\n",
    "        offset_max (int): The maximum offset to scrape.\n",
    "        offset_step (int): The step size for the offset.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of URLs to scrape.\n",
    "    \"\"\"\n",
    "    catalogue_urls = [f\"{catalogue_base_url}&offset={i}\" for i in range(0, offset_max, offset_step)]\n",
    "    franchise_urls = []\n",
    "    for catalogue_url in catalogue_urls:\n",
    "        franchise_urls.extend(get_franchise_url(session, base_url, catalogue_url))\n",
    "        break\n",
    "    return franchise_urls\n",
    "\n",
    "\n",
    "def get_franchise_data(session: requests.Session, url: str) -> dict:\n",
    "    resp = session.get(url)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    td = soup.find_all(\"td\", attrs={\"colspan\": \"2\"})[1]\n",
    "\n",
    "    return td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3020bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_franchise_data(session, \"https://franservesupport.com/franchisedetails.asp?FranID=4017&ClientID=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a32d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ca0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slugify(text: str) -> str:\n",
    "    \"\"\"Converts a string into a URL-friendly slug.\"\"\"\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[\\s/()]+\", \"_\", text)\n",
    "    text = re.sub(r\"[^a-z0-9_]\", \"\", text)\n",
    "    text = text.strip(\"_\")\n",
    "    return text\n",
    "\n",
    "text = \"aaa-aaa\"\n",
    "slugify(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ebfd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def parse_franchise_html(soup):\n",
    "    \"\"\"\n",
    "    Parses the HTML content of a FranServe franchise page and extracts the data.\n",
    "\n",
    "    Args:\n",
    "        soup: The HTML content of the franchise page as a string.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the extracted franchise and contact information.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_text_or_none(element):\n",
    "        \"\"\"Returns the text of an element or None if the element doesn't exist.\"\"\"\n",
    "        return element.get_text(strip=True) if element else None\n",
    "\n",
    "    def clean_financial_value(text):\n",
    "        \"\"\"Removes currency symbols and commas and converts to integer.\"\"\"\n",
    "        if not text:\n",
    "            return None\n",
    "        return int(re.sub(r'[$,]', '', text.split()[0]))\n",
    "\n",
    "    # --- Franchise Information ---\n",
    "    franchise_data = {}\n",
    "\n",
    "    franchise_name_tag = soup.find('b').find('font', size='+1')\n",
    "    franchise_data['franchise_name'] = get_text_or_none(franchise_name_tag)\n",
    "\n",
    "    fran_id_tag = soup.find('input', {'name': 'ZorID'})\n",
    "    franchise_data['source_id'] = int(fran_id_tag['value']) if fran_id_tag else None\n",
    "    \n",
    "    franchise_data['source_url'] = f\"https://www.franserve.com/franchise.asp?id={franchise_data['source_id']}\" if franchise_data['source_id'] else None\n",
    "\n",
    "    category_links = soup.select('div.col-left > div > b:contains(\"Category:\") ~ a')\n",
    "    franchise_data['primary_category'] = get_text_or_none(category_links[0]) if category_links else None\n",
    "\n",
    "    sub_category_links = soup.select('div.col-left > div > b:contains(\"Subcategory:\") ~ a')\n",
    "    franchise_data['sub_categories'] = json.dumps([get_text_or_none(link) for link in sub_category_links]) if sub_category_links else None\n",
    "\n",
    "    left_col_text = soup.select_one('div.col-left > div').get_text()\n",
    "    \n",
    "    corporate_address_match = re.search(r'Corporate Office:\\s*(.*?)(?=\\s*Contact:)', left_col_text, re.DOTALL)\n",
    "    franchise_data['corporate_address'] = corporate_address_match.group(1).strip() if corporate_address_match else None\n",
    "\n",
    "    website_tag = soup.find('a', href=re.compile(r'www\\..*'))\n",
    "    franchise_data['website_url'] = website_tag['href'] if website_tag else None\n",
    "    \n",
    "    right_col_top_div = soup.select_one('div.col-left > div:nth-of-type(2)')\n",
    "    if right_col_top_div:\n",
    "        right_col_top_text = right_col_top_div.get_text()\n",
    "        franchise_data['franchise_fee_usd'] = clean_financial_value(re.search(r'Franchise Fee:\\s*\\$([\\d,]+)', right_col_top_text).group(1) if re.search(r'Franchise Fee:\\s*\\$([\\d,]+)', right_col_top_text) else None)\n",
    "        franchise_data['required_cash_investment_usd'] = clean_financial_value(re.search(r'Cash Investment:\\s*\\$([\\d,]+)', right_col_top_text).group(1) if re.search(r'Cash Investment:\\s*\\$([\\d,]+)', right_col_top_text) else None)\n",
    "        \n",
    "        total_investment_match = re.search(r'Total Investment:\\s*\\$([\\d,]+)\\s*-\\s*\\$([\\d,]+)', right_col_top_text)\n",
    "        if total_investment_match:\n",
    "            franchise_data['total_investment_min_usd'] = clean_financial_value(total_investment_match.group(1))\n",
    "            franchise_data['total_investment_max_usd'] = clean_financial_value(total_investment_match.group(2))\n",
    "        \n",
    "        franchise_data['required_net_worth_usd'] = clean_financial_value(re.search(r'NetWorth:\\s*\\$([\\d,]+)', right_col_top_text).group(1) if re.search(r'NetWorth:\\s*\\$([\\d,]+)', right_col_top_text) else None)\n",
    "        franchise_data['royalty_details_text'] = re.search(r'Royalties:\\s*(.+)', right_col_top_text).group(1).strip() if re.search(r'Royalties:\\s*(.+)', right_col_top_text) else None\n",
    "        \n",
    "        sba_approved_text = re.search(r'SBA approved:\\s*(.*)', right_col_top_text)\n",
    "        franchise_data['sba_approved'] = 'Yes' in sba_approved_text.group(1) if sba_approved_text and sba_approved_text.group(1).strip() else False\n",
    "        \n",
    "        vetfran_text = re.search(r'VetFran:\\s*(.*)', right_col_top_text)\n",
    "        franchise_data['vetfran_member'] = 'Yes' in vetfran_text.group(1) if vetfran_text else False\n",
    "        \n",
    "        master_franchise_text = re.search(r'Master Franchise / Area Developer Opportunity:\\s*(.*)', right_col_top_text)\n",
    "        franchise_data['master_franchise_opportunity'] = 'Yes' in master_franchise_text.group(1) if master_franchise_text else False\n",
    "        \n",
    "        founded_match = re.search(r'Founded:\\s*(\\d{4})', right_col_top_text)\n",
    "        franchise_data['founded_year'] = int(founded_match.group(1)) if founded_match else None\n",
    "        \n",
    "        franchised_match = re.search(r'Franchised:\\s*(\\d{4})', right_col_top_text)\n",
    "        franchise_data['franchised_year'] = int(franchised_match.group(1)) if franchised_match else None\n",
    "\n",
    "    additional_details_heading = soup.find('h2', string='Additional Details')\n",
    "    description_paragraphs = []\n",
    "    if additional_details_heading:\n",
    "        for sibling in additional_details_heading.find_next_siblings():\n",
    "            if sibling.name == 'table':\n",
    "                break\n",
    "            if sibling.name == 'p':\n",
    "                description_paragraphs.append(get_text_or_none(sibling))\n",
    "    franchise_data['description_text'] = '\\n'.join(description_paragraphs).strip()\n",
    "\n",
    "    why_franchise_list = soup.select('p:contains(\"WHY\") + ul li')\n",
    "    franchise_data['why_franchise_summary'] = json.dumps([get_text_or_none(li) for li in why_franchise_list]) if why_franchise_list else None\n",
    "\n",
    "    ideal_candidate_list = soup.select('p:contains(\"IDEAL FRANCHISEE\") + ul li')\n",
    "    franchise_data['ideal_candidate_profile_text'] = json.dumps([get_text_or_none(li) for li in ideal_candidate_list]) if ideal_candidate_list else None\n",
    "    \n",
    "    background_section_text = soup.select_one('td:contains(\"BACKGROUND\")').get_text() if soup.select_one('td:contains(\"BACKGROUND\")') else ''\n",
    "    \n",
    "    home_based_match = re.search(r'Home Based:\\s*(Yes|No)', background_section_text)\n",
    "    franchise_data['is_home_based'] = home_based_match.group(1) == 'Yes' if home_based_match else None\n",
    "    \n",
    "    semi_absentee_match = re.search(r'Semi-Absentee ownership available:\\s*(Yes|No)', background_section_text)\n",
    "    franchise_data['allows_semi_absentee'] = semi_absentee_match.group(1) == 'Yes' if semi_absentee_match else None\n",
    "    \n",
    "    absentee_match = re.search(r'Absentee ownership available:\\s*(Yes|No)', background_section_text)\n",
    "    franchise_data['allows_absentee'] = absentee_match.group(1) == 'Yes' if absentee_match else None\n",
    "    \n",
    "    e2_visa_match = re.search(r'E2 Visa Friendly:\\s*(Yes|No)', background_section_text)\n",
    "    franchise_data['e2_visa_friendly'] = e2_visa_match.group(1) == 'Yes' if e2_visa_match else None\n",
    "    \n",
    "    unavailable_states_match = re.search(r'NOT available:\\s*(.+)', background_section_text)\n",
    "    if unavailable_states_match:\n",
    "        states_text = unavailable_states_match.group(1).strip()\n",
    "        unavailable_states = [state.strip() for state in states_text.split(', ')]\n",
    "        franchise_data['unavailable_states'] = json.dumps(unavailable_states)\n",
    "    else:\n",
    "        franchise_data['unavailable_states'] = None\n",
    "\n",
    "    num_franchises_match = re.search(r'Number of franchises currently operating:\\s*(\\d+)', background_section_text)\n",
    "    num_international_match = re.search(r'Number of International franchises currently operating:\\s*(\\d+)', background_section_text)\n",
    "    num_corporate_match = re.search(r'Number of corporate owned franchises:\\s*(\\d+)', background_section_text)\n",
    "\n",
    "    franchise_data['locations'] = json.dumps({\n",
    "        'operating_franchises': int(num_franchises_match.group(1)) if num_franchises_match else 0,\n",
    "        'international_franchises': int(num_international_match.group(1)) if num_international_match else 0,\n",
    "        'corporate_owned': int(num_corporate_match.group(1)) if num_corporate_match else 0\n",
    "    })\n",
    "\n",
    "    last_updated_tag = soup.find('i', string=re.compile(r'Last updated:'))\n",
    "    franchise_data['last_updated_from_source'] = get_text_or_none(last_updated_tag).replace('Last updated: ', '').strip() if last_updated_tag else None\n",
    "\n",
    "    vetfran_discount_details_match = re.search(r'Veterans/Minorities/First Responders Discount\\?\\s*(Yes|No)', background_section_text)\n",
    "    if vetfran_discount_details_match and vetfran_discount_details_match.group(1) == 'Yes':\n",
    "        franchise_data['vetfran_discount_details'] = 'Discount available for Veterans/Minorities/First Responders.'\n",
    "    else:\n",
    "        franchise_data['vetfran_discount_details'] = None\n",
    "\n",
    "    # --- Contacts Information ---\n",
    "    contacts_data = []\n",
    "    \n",
    "    # Primary Contact\n",
    "    primary_contact = {}\n",
    "    contact_name_tag = soup.find('b', string='Contact:')\n",
    "    if contact_name_tag:\n",
    "        primary_contact['name'] = contact_name_tag.next_sibling.strip()\n",
    "        primary_contact['phone'] = get_text_or_none(contact_name_tag.find_next('b', string='Phone:').next_sibling)\n",
    "        email_tag = contact_name_tag.find_next('a', href=lambda href: href and 'mailto:' in href)\n",
    "        primary_contact['email'] = get_text_or_none(email_tag)\n",
    "        primary_contact['is_primary'] = True\n",
    "        primary_contact['notes'] = None\n",
    "        contacts_data.append(primary_contact)\n",
    "\n",
    "    # Alternative Contact\n",
    "    alt_contact = {}\n",
    "    alt_contact_name_tag = soup.find('b', string='Alternative Contact:')\n",
    "    if alt_contact_name_tag:\n",
    "        alt_contact['name'] = alt_contact_name_tag.next_sibling.strip()\n",
    "        phone_after_alt = alt_contact_name_tag.find_next('b', string='Phone:')\n",
    "        alt_contact['phone'] = get_text_or_none(phone_after_alt.next_sibling)\n",
    "        email_after_alt = alt_contact_name_tag.find_next('a', href=lambda href: href and 'mailto:' in href)\n",
    "        alt_contact['email'] = get_text_or_none(email_after_alt)\n",
    "        alt_contact['is_primary'] = False\n",
    "        alt_contact['notes'] = 'Alternative Contact'\n",
    "        contacts_data.append(alt_contact)\n",
    "        \n",
    "    return {\n",
    "        'franchise_data': franchise_data,\n",
    "        'contacts_data': contacts_data\n",
    "    }\n",
    "\n",
    "parse_franchise_html(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_franchises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
